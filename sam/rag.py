import glob
import faiss
import numpy as np
import pathlib
from pathlib import Path
import requests
import os
import re
from config.settings import Config
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from langchain_community.document_loaders import PyPDFLoader
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from requests.auth import HTTPBasicAuth
import urllib3
import json
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
from openai import OpenAI

# Importer l'exception Streamlit pour les secrets
try:
    from streamlit.errors import StreamlitSecretNotFoundError
except ImportError:
    # Pour les versions plus anciennes de Streamlit
    StreamlitSecretNotFoundError = Exception

# Charger le .env depuis la racine du projet
env_path = Path(__file__).parent.parent / '.env'
if env_path.exists():
    load_dotenv(env_path)
else:
    load_dotenv()  # Fallback to default .env location

# Variable globale pour le client OpenAI (initialis√©e de mani√®re paresseuse)
_client = None

def get_openai_client():
    """Obtient le client OpenAI, en le cr√©ant si n√©cessaire."""
    global _client
    if _client is None:
        # Essayer d'abord Streamlit secrets (pour production)
        api_key = None
        try:
            import streamlit as st
            if hasattr(st, 'secrets'):
                try:
                    # Tenter d'acc√©der aux secrets (peut lever StreamlitSecretNotFoundError)
                    secrets = st.secrets
                    if 'OPENAI_API_KEY' in secrets:
                        api_key = secrets['OPENAI_API_KEY']
                    elif 'openai' in secrets and 'api_key' in secrets['openai']:
                        api_key = secrets['openai']['api_key']
                except StreamlitSecretNotFoundError:
                    # Fichier secrets.toml non trouv√©, utiliser .env
                    pass
                except (KeyError, AttributeError, TypeError):
                    # Erreur lors de l'acc√®s aux secrets, utiliser .env
                    pass
        except ImportError:
            pass
        
        # Sinon, utiliser les variables d'environnement
        if not api_key:
            api_key = os.getenv("OPENAI_API_KEY")
        
        if not api_key:
            raise ValueError(
                "OPENAI_API_KEY n'est pas d√©finie. "
                "Veuillez configurer cette variable d'environnement dans le fichier .env "
                "ou dans les param√®tres de Streamlit Cloud (secrets)."
            )
        _client = OpenAI(api_key=api_key)
    return _client
# Configuration
from_code = "en"
to_code = "fr"
#AUTH_URL = Config.AUTH_URL
#API_URL = Config.API_URL
#USERNAME = Config.USER
#PASSWORD = Config.PASSWORD
MODEL_DIR = Config.MODEL_DIR or "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
#MODEL_ID = Config.MODEL_ID

def save_chunks(chunks, filepath):
    """Sauvegarder les chunks dans un fichier JSON."""
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump([doc.page_content for doc in chunks], f, indent=4)

def load_chunks(filepath):
    """Charger les chunks √† partir d'un fichier JSON."""
    with open(filepath, 'r', encoding='utf-8') as f:
        content = json.load(f)
    return [Document(page_content=chunk) for chunk in content]

# Load documents and create FAISS index
def load_documents_and_create_chunks():
    """Charger les documents, les traduire et les diviser en chunks."""
    
    # Obtenir le r√©pertoire du fichier rag.py
    rag_dir = os.path.dirname(os.path.abspath(__file__))
    chunks_filepath = os.path.join(rag_dir, "chunks.json")
    
    if os.path.exists(chunks_filepath):
        print("Chargement des chunks √† partir du fichier.")
        chunks = load_chunks(chunks_filepath)
        print(f"‚úÖ {len(chunks)} chunks charg√©s depuis le fichier")
        return chunks

    documents = []
    print("start load doc in document")
    
    contrat_dir = os.path.join(rag_dir, "contrat")
    
    # V√©rifier que le dossier existe
    if not os.path.exists(contrat_dir):
        raise FileNotFoundError(f"‚ùå Le dossier 'contrat' n'existe pas! Chemin recherch√©: {contrat_dir}")
    
    # V√©rifier qu'il y a des fichiers PDF
    pdf_files = glob.glob(os.path.join(contrat_dir, "*.pdf"))
    if not pdf_files:
        raise FileNotFoundError("‚ùå Aucun fichier PDF trouv√© dans le dossier 'contrat'!")
    
    print(f"üìÑ {len(pdf_files)} fichiers PDF trouv√©s")
    
    for file in pdf_files:
        try:
            print(f"  Chargement de {file}...")
            loader = PyPDFLoader(file)
            documents += loader.load()
        except Exception as e:
            print(f"‚ùå Erreur survenue pour le fichier '{file}': {e}")
    
    if not documents:
        raise ValueError("‚ùå Aucun document n'a pu √™tre charg√©!")
    
    print(f"‚úÖ {len(documents)} documents charg√©s")
    print("finish load doc in document")
    print("start the translate")
    print("finish the translate")   
    
    # Diviser les documents en chunks
    print("start the splitting of the text")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=600, 
        chunk_overlap=120, 
        length_function=len, 
        separators=["\n\n", "\n", "."]
    )
    chunks = text_splitter.split_documents(documents=documents)
    
    if not chunks:
        raise ValueError("‚ùå Aucun chunk cr√©√© apr√®s le splitting!")
    
    print(f"‚úÖ {len(chunks)} chunks cr√©√©s")
    print("finish the splitting of the text")
    
    save_chunks(chunks, chunks_filepath)
    return chunks

def initialize_chatbot():
    faiss.omp_set_num_threads(3)
    # Obtenir le r√©pertoire du fichier rag.py
    rag_dir = os.path.dirname(os.path.abspath(__file__))
    index_path = os.path.join(rag_dir, "indexFaiss", "local_index.faiss")
    
    # Toujours cr√©er les chunks
    print("start loading document and create chunks")
    chunks = load_documents_and_create_chunks()
    
    if not chunks:
        raise ValueError("‚ùå Aucun chunk disponible!")
    
    print(f"‚úÖ {len(chunks)} chunks disponibles")
    print("finish loading document and create chunks")
    
    print("ü§ñ Chargement du mod√®le d'embeddings...")
    emb = HuggingFaceEmbeddings(
        model_name=MODEL_DIR, 
        encode_kwargs={"normalize_embeddings": True}
    )

    if os.path.exists(index_path):
        # Charger l'index directement depuis le fichier FAISS
        index = faiss.read_index(index_path)
        print(f"‚úÖ Index charg√© depuis le fichier existant ({index.ntotal} vecteurs)")
    else:
        # Cr√©er un nouvel index
        print("start the creation of the faiss index")
        index = create_faiss_index(chunks, emb)
        print("finish the creation of the faiss index")
    
    return chunks, emb, index

def create_faiss_index(chunks, emb):
    """Cr√©er un index FAISS √† partir des chunks."""
    
    # V√âRIFICATIONS CRITIQUES
    if not chunks:
        raise ValueError("‚ùå Liste de chunks vide!")
    
    print(f"üîÑ G√©n√©ration des embeddings pour {len(chunks)} chunks...")
    
    # Extraire le texte des chunks
    chunk_texts = [chunk.page_content for chunk in chunks]
    
    # V√©rifier qu'il y a du contenu
    if not chunk_texts or not chunk_texts[0]:
        raise ValueError("‚ùå Les chunks ne contiennent pas de texte!")
    
    print(f"üìù Premier chunk (100 premiers caract√®res): {chunk_texts[0][:100]}...")
    
    # G√©n√©rer les embeddings en batch (plus efficace)
    try:
        chunk_vectors = emb.embed_documents(chunk_texts)
    except Exception as e:
        print(f"‚ùå Erreur lors de la g√©n√©ration des embeddings: {e}")
        raise
    
    # V√©rifier que les embeddings ont √©t√© g√©n√©r√©s
    if not chunk_vectors or len(chunk_vectors) == 0:
        raise ValueError("‚ùå Aucun embedding g√©n√©r√©!")
    
    print(f"‚úÖ {len(chunk_vectors)} embeddings g√©n√©r√©s")
    print(f"‚úÖ Dimension des embeddings: {len(chunk_vectors[0])}")
    
    # Convertir en array numpy
    chunk_vectors_array = np.array(chunk_vectors).astype('float32')
    
    # Cr√©er l'index FAISS
    dimension = chunk_vectors_array.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(chunk_vectors_array)
    
    print(f"‚úÖ Index FAISS cr√©√© avec {index.ntotal} vecteurs")
    
    # Obtenir le r√©pertoire du fichier rag.py
    rag_dir = os.path.dirname(os.path.abspath(__file__))
    index_dir = os.path.join(rag_dir, "indexFaiss")
    
    # Cr√©er le dossier si n√©cessaire
    os.makedirs(index_dir, exist_ok=True)
    
    # Sauvegarder l'index
    index_path = os.path.join(index_dir, "local_index.faiss")
    faiss.write_index(index, index_path)
    print(f"‚úÖ Index sauvegard√© dans '{index_path}'")
    
    return index

def search_faiss_index(query, emb, index, k=5):
    print("start vectorisation de la requete")
    query_vector = np.array(emb.embed_query(query)).astype('float32')
    print("finish vectorisation de la requete")
    print("start research of index by similarity")
    distances, indices = index.search(np.array([query_vector]), k)
    print("finish research of index by similarity")
    return indices, distances

# Use the LLM API
def use_llm(prompt_text):
    try:
        system_instruction = (
            "Tu es un assistant AI nomm√© Mosam con√ßu pour aider des douaniers √† troiuver les prix √† fixer sur les produits "
            "RGI 1: Les titres des sections, chapitres et sous-chapitres n'ont qu'une valeur indicative."

"RGI 2: Les marchandises incompl√®tes ou non finies sont class√©es comme compl√®tes."

"RGI 3: Le m√©lange ou l'assemblage de mati√®res ou d'articles est class√© selon la mati√®re pr√©pond√©rante."

"RGI 4: Les marchandises qui ne peuvent √™tre class√©es selon les r√®gles 1 √† 3 sont class√©es dans la position la plus analogue."

"RGI 5: Les emballages sont class√©s avec les marchandises qu'ils contiennent."

"RGI 6: Le classement des marchandises dans les sous-positions d'une m√™me position est d√©termin√© selon les termes de ces sous-positions."
            "R√©ponds uniquement aux questions pos√©es. Tu ne peux pas fournir de r√©ponse si "
            "l'information n'est pas explicitement pr√©sente dans les documents. "
            "Mosam est une √©quipe juridique, donc tu ne peux pas inventer de r√©ponse. "
            "Priorise la clart√© et la concision. "
            "Ne mentionne jamais des documents : fais comme si les infos √©taient internes. "
            "Si la question sort du p√©rim√®tre Mosam, rappelle le p√©rim√®tre. "
            "Si tu n'as pas la r√©ponse, demande des pr√©cisions. "
            "R√©ponds dans la langue du prompt suivant. "
            "Chaque r√©ponse doit, pour chaque marchandise, indiquer explicitement la position tarifaire "
            "du TEC/SH, le ou les taux d'imposition applicables (droits de douane, TVA, autres taxes "
            "si disponibles) et une justification synth√©tique (RGI, notes, crit√®res techniques). "
            "S'il y a plusieurs marchandises, structure la r√©ponse sous forme de tableau ou de liste "
            "s√©par√©e, une ligne par marchandise. "
            "Si le terme exact n'appara√Æt pas dans les documents, tente imm√©diatement plusieurs synonymes "
            "ou variantes (par exemple: \"barre m√©tallique\", \"barre en fer\", \"barre en acier\", "
            "\"produit sid√©rurgique\"). "
            "Si malgr√© ces variantes aucune mention explicite n'est trouv√©e, r√©alise une d√©duction en t'appuyant "
            "sur les RGI et sur la logique du TEC CEDEAO, en signalant clairement qu'il s'agit d'une "
            "d√©duction fond√©e sur les r√®gles."
            "Dans les documents sources, interpr√®te les abr√©viations suivantes: "
            "\"D.D.\" = droits de douane, \"R.S.\" = r√©gime statistique (taxe statistique), "
            "\"U.S.\" = unit√© de mesure et \"N.T.S.\" = num√©ro tarifaire suppl√©mentaire. "
            "Quand tu cites ces abr√©viations, ajoute toujours la d√©finition entre parenth√®ses, "
            "par exemple \"D.D. (droits de douane)\". "
            "Retourne exclusivement un objet JSON unique (aucun texte en dehors du JSON) respectant le sch√©ma "
            "suivant: {\"narrative\":\"texte synth√©tique pour le douanier\",\"classifications\":[{"
            "\"description\":\"R√©sum√© de la marchandise\",\"hs_code\":\"8517.13.00.00\","
            "\"section\":\"XVI\",\"chapter\":\"85\",\"dd_rate\":\"5 %\",\"rs_rate\":\"1 %\","
            "\"us_unit\":\"PI√àCE\",\"other_taxes\":\"TVA 18 %\","
            "\"justification\":\"Synth√®se RGI / crit√®res\",\"excerpt\":\"Citation exacte du document\","
            "\"origin\":\"USA\",\"value\":\"Non renseign√©\",\"confidence\":92}]}. "
            "Chaque objet de \"classifications\" doit contenir au minimum ces champs; utilise "
            "\"Non renseign√©\" si une donn√©e manque et veille √† ce que \"chapter\" soit toujours sur deux chiffres "
            "et \"confidence\" un nombre entre 0 et 100. Le champ \"description\" doit reprendre le nom pr√©cis "
            "du produit class√© tel qu'√©nonc√© par le douanier (ou une reformulation tr√®s courte), afin de pouvoir "
            "l'afficher directement dans le tableau de suivi. "
            "Si tu dois faire une d√©duction, indique-le dans \"justification\"."
        )

        client = get_openai_client()
        response = client.responses.create(
            model="gpt-5-nano",
            input=[
                {"role": "system", "content": system_instruction},
                {"role": "user", "content": prompt_text}
            ],
            store=True
        )

        return response.output_text

    except Exception as e:
        return f"Erreur lors de l'appel au mod√®le OpenAI : {e}"

def split_user_queries(raw_text):
    """D√©coupe l'entr√©e utilisateur si plusieurs articles sont fournis d'un coup."""
    if not raw_text:
        return []
    normalized = raw_text.replace("\r", "\n")
    line_parts = [
        re.sub(r"^[\-\*\d\)\.]+\s*", "", line).strip()
        for line in normalized.split("\n")
    ]
    queries = [part for part in line_parts if part]
    if len(queries) > 1:
        return queries
    if ";" in raw_text:
        semi_parts = [seg.strip() for seg in raw_text.split(";") if seg.strip()]
        if len(semi_parts) > 1:
            return semi_parts
    return [raw_text.strip()] if raw_text.strip() else []


def build_context_for_query(query, chunks, emb, index):
    """G√©n√®re un contexte documentaire pour une requ√™te pr√©cise."""
    indices, _ = search_faiss_index(query, emb, index)
    context = ""
    for idx in indices[0]:
        context += chunks[idx].page_content + "\n"
    return context


def process_user_input(user_input, chunks, emb, index):
    queries = split_user_queries(user_input)
    if not queries:
        return "Merci de pr√©ciser au moins une marchandise √† classifier."

    prompt_sections = []
    for i, query in enumerate(queries, start=1):
        context = build_context_for_query(query, chunks, emb, index)
        prompt_sections.append(
            f"[MARCHANDISE {i}]\nDescription: {query}\nContexte documentaire:\n{context}"
        )

    combined_context = "\n\n".join(prompt_sections)
    enriched_prompt = (
        "Le douanier peut avoir fourni plusieurs marchandises. "
        "Analyse chaque bloc ci-dessous et produis une r√©ponse structur√©e avec, pour chaque marchandise, "
        "la position tarifaire, le taux d'imposition et les d√©tails pertinents.\n\n"
        f"{combined_context}\n\nDemande initiale du douanier:\n{user_input}"
    )
    print("start the send of the question")
    response = use_llm(enriched_prompt)
    print("finish the send of the question")
    return response