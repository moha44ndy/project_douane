import glob
import faiss
import numpy as np
import pathlib
from pathlib import Path
import requests
import os
import re
import hashlib
from config.settings import Config
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from langchain_community.document_loaders import PyPDFLoader
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from requests.auth import HTTPBasicAuth
import urllib3
import json
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
from openai import OpenAI

# Importer l'exception Streamlit pour les secrets
try:
    from streamlit.errors import StreamlitSecretNotFoundError
except ImportError:
    # Pour les versions plus anciennes de Streamlit
    StreamlitSecretNotFoundError = Exception

# Importer le module de cache Redis
try:
    from cache_redis import (
        get_from_cache,
        set_to_cache,
        delete_from_cache,
        clear_cache,
        get_cache_stats,
        is_redis_available
    )
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False
    print("‚ö†Ô∏è Module cache_redis non disponible, utilisation du cache local")

# Charger le .env depuis la racine du projet
env_path = Path(__file__).parent.parent / '.env'
if env_path.exists():
    load_dotenv(env_path)
else:
    load_dotenv()  # Fallback to default .env location

# Variable globale pour le client OpenAI (initialis√©e de mani√®re paresseuse)
_client = None

# Cache local de fallback (si Redis n'est pas disponible)
# Utiliser session_state pour persister entre les pages
_local_cache = None

def get_local_cache():
    """Obtient le cache local depuis session_state (fallback si Redis n'est pas disponible)"""
    import streamlit as st
    global _local_cache
    if "_local_cache" not in st.session_state:
        st.session_state["_local_cache"] = {}
    return st.session_state["_local_cache"]

def get_openai_client():
    """Obtient le client OpenAI, en le cr√©ant si n√©cessaire."""
    global _client
    if _client is None:
        # Essayer d'abord Streamlit secrets (pour production)
        api_key = None
        try:
            import streamlit as st
            if hasattr(st, 'secrets'):
                try:
                    # Tenter d'acc√©der aux secrets (peut lever StreamlitSecretNotFoundError)
                    secrets = st.secrets
                    if 'OPENAI_API_KEY' in secrets:
                        api_key = secrets['OPENAI_API_KEY']
                    elif 'openai' in secrets and 'api_key' in secrets['openai']:
                        api_key = secrets['openai']['api_key']
                except StreamlitSecretNotFoundError:
                    # Fichier secrets.toml non trouv√©, utiliser .env
                    pass
                except (KeyError, AttributeError, TypeError):
                    # Erreur lors de l'acc√®s aux secrets, utiliser .env
                    pass
        except ImportError:
            pass
        
        # Sinon, utiliser les variables d'environnement
        if not api_key:
            api_key = os.getenv("OPENAI_API_KEY")
        
        if not api_key:
            raise ValueError(
                "OPENAI_API_KEY n'est pas d√©finie. "
                "Veuillez configurer cette variable d'environnement dans le fichier .env "
                "ou dans les param√®tres de Streamlit Cloud (secrets)."
            )
        _client = OpenAI(api_key=api_key)
    return _client
# Configuration
from_code = "en"
to_code = "fr"
#AUTH_URL = Config.AUTH_URL
#API_URL = Config.API_URL
#USERNAME = Config.USER
#PASSWORD = Config.PASSWORD
MODEL_DIR = Config.MODEL_DIR or "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
#MODEL_ID = Config.MODEL_ID

def save_chunks(chunks, filepath):
    """Sauvegarder les chunks dans un fichier JSON."""
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump([doc.page_content for doc in chunks], f, indent=4)

def load_chunks(filepath):
    """Charger les chunks √† partir d'un fichier JSON."""
    with open(filepath, 'r', encoding='utf-8') as f:
        content = json.load(f)
    return [Document(page_content=chunk) for chunk in content]

# Load documents and create FAISS index
def load_documents_and_create_chunks():
    """Charger les documents, les traduire et les diviser en chunks."""
    
    # Obtenir le r√©pertoire du fichier rag.py
    rag_dir = os.path.dirname(os.path.abspath(__file__))
    chunks_filepath = os.path.join(rag_dir, "chunks.json")
    
    if os.path.exists(chunks_filepath):
        print("Chargement des chunks √† partir du fichier.")
        chunks = load_chunks(chunks_filepath)
        print(f"‚úÖ {len(chunks)} chunks charg√©s depuis le fichier")
        return chunks

    documents = []
    print("start load doc in document")
    
    contrat_dir = os.path.join(rag_dir, "contrat")
    
    # V√©rifier que le dossier existe
    if not os.path.exists(contrat_dir):
        raise FileNotFoundError(f"‚ùå Le dossier 'contrat' n'existe pas! Chemin recherch√©: {contrat_dir}")
    
    # V√©rifier qu'il y a des fichiers PDF
    pdf_files = glob.glob(os.path.join(contrat_dir, "*.pdf"))
    if not pdf_files:
        raise FileNotFoundError("‚ùå Aucun fichier PDF trouv√© dans le dossier 'contrat'!")
    
    print(f"üìÑ {len(pdf_files)} fichiers PDF trouv√©s")
    
    for file in pdf_files:
        try:
            print(f"  Chargement de {file}...")
            loader = PyPDFLoader(file)
            documents += loader.load()
        except Exception as e:
            print(f"‚ùå Erreur survenue pour le fichier '{file}': {e}")
    
    if not documents:
        raise ValueError("‚ùå Aucun document n'a pu √™tre charg√©!")
    
    print(f"‚úÖ {len(documents)} documents charg√©s")
    print("finish load doc in document")
    print("start the translate")
    print("finish the translate")   
    
    # Diviser les documents en chunks
    print("start the splitting of the text")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=600, 
        chunk_overlap=120, 
        length_function=len, 
        separators=["\n\n", "\n", "."]
    )
    chunks = text_splitter.split_documents(documents=documents)
    
    if not chunks:
        raise ValueError("‚ùå Aucun chunk cr√©√© apr√®s le splitting!")
    
    print(f"‚úÖ {len(chunks)} chunks cr√©√©s")
    print("finish the splitting of the text")
    
    save_chunks(chunks, chunks_filepath)
    return chunks

def initialize_chatbot():
    faiss.omp_set_num_threads(3)
    # Obtenir le r√©pertoire du fichier rag.py
    rag_dir = os.path.dirname(os.path.abspath(__file__))
    index_path = os.path.join(rag_dir, "indexFaiss", "local_index.faiss")
    
    # Toujours cr√©er les chunks
    print("start loading document and create chunks")
    chunks = load_documents_and_create_chunks()
    
    if not chunks:
        raise ValueError("‚ùå Aucun chunk disponible!")
    
    print(f"‚úÖ {len(chunks)} chunks disponibles")
    print("finish loading document and create chunks")
    
    print("ü§ñ Chargement du mod√®le d'embeddings...")
    try:
        # Optimiser le chargement du mod√®le pour r√©duire l'utilisation m√©moire
        emb = HuggingFaceEmbeddings(
            model_name=MODEL_DIR, 
            encode_kwargs={"normalize_embeddings": True},
            model_kwargs={"device": "cpu", "trust_remote_code": False}  # Forcer l'utilisation du CPU
        )
    except OSError as e:
        if "1455" in str(e) or "pagination" in str(e).lower() or "paging" in str(e).lower():
            error_msg = """
            ‚ùå ERREUR DE M√âMOIRE VIRTUELLE
            
            Le fichier de pagination Windows est insuffisant pour charger le mod√®le.
            
            Solutions :
            1. Augmenter le fichier de pagination Windows :
               - Panneau de configuration > Syst√®me > Param√®tres syst√®me avanc√©s
               - Performance > Param√®tres > Avanc√© > M√©moire virtuelle
               - Augmenter la taille du fichier d'√©change (recommand√© : 2x la RAM)
            
            2. Fermer d'autres applications pour lib√©rer de la m√©moire
            
            3. Red√©marrer l'ordinateur pour lib√©rer la m√©moire
            """
            raise RuntimeError(error_msg) from e
        else:
            raise

    if os.path.exists(index_path):
        # Charger l'index directement depuis le fichier FAISS
        index = faiss.read_index(index_path)
        print(f"‚úÖ Index charg√© depuis le fichier existant ({index.ntotal} vecteurs)")
    else:
        # Cr√©er un nouvel index
        print("start the creation of the faiss index")
        index = create_faiss_index(chunks, emb)
        print("finish the creation of the faiss index")
    
    return chunks, emb, index

def create_faiss_index(chunks, emb):
    """Cr√©er un index FAISS √† partir des chunks."""
    
    # V√âRIFICATIONS CRITIQUES
    if not chunks:
        raise ValueError("‚ùå Liste de chunks vide!")
    
    print(f"üîÑ G√©n√©ration des embeddings pour {len(chunks)} chunks...")
    
    # Extraire le texte des chunks
    chunk_texts = [chunk.page_content for chunk in chunks]
    
    # V√©rifier qu'il y a du contenu
    if not chunk_texts or not chunk_texts[0]:
        raise ValueError("‚ùå Les chunks ne contiennent pas de texte!")
    
    print(f"üìù Premier chunk (100 premiers caract√®res): {chunk_texts[0][:100]}...")
    
    # G√©n√©rer les embeddings en batch (plus efficace)
    try:
        chunk_vectors = emb.embed_documents(chunk_texts)
    except Exception as e:
        print(f"‚ùå Erreur lors de la g√©n√©ration des embeddings: {e}")
        raise
    
    # V√©rifier que les embeddings ont √©t√© g√©n√©r√©s
    if not chunk_vectors or len(chunk_vectors) == 0:
        raise ValueError("‚ùå Aucun embedding g√©n√©r√©!")
    
    print(f"‚úÖ {len(chunk_vectors)} embeddings g√©n√©r√©s")
    print(f"‚úÖ Dimension des embeddings: {len(chunk_vectors[0])}")
    
    # Convertir en array numpy
    chunk_vectors_array = np.array(chunk_vectors).astype('float32')
    
    # Cr√©er l'index FAISS
    dimension = chunk_vectors_array.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(chunk_vectors_array)
    
    print(f"‚úÖ Index FAISS cr√©√© avec {index.ntotal} vecteurs")
    
    # Obtenir le r√©pertoire du fichier rag.py
    rag_dir = os.path.dirname(os.path.abspath(__file__))
    index_dir = os.path.join(rag_dir, "indexFaiss")
    
    # Cr√©er le dossier si n√©cessaire
    os.makedirs(index_dir, exist_ok=True)
    
    # Sauvegarder l'index
    index_path = os.path.join(index_dir, "local_index.faiss")
    faiss.write_index(index, index_path)
    print(f"‚úÖ Index sauvegard√© dans '{index_path}'")
    
    return index

def search_faiss_index(query, emb, index, k=5):
    print("start vectorisation de la requete")
    query_vector = np.array(emb.embed_query(query)).astype('float32')
    print("finish vectorisation de la requete")
    print("start research of index by similarity")
    distances, indices = index.search(np.array([query_vector]), k)
    print("finish research of index by similarity")
    return indices, distances

# Use the LLM API
def use_llm(prompt_text, user_query=None):
    """
    Utilise le LLM pour g√©n√©rer une r√©ponse
    user_query: La requ√™te originale de l'utilisateur (pour validation pr√©ventive)
    """
    # Utiliser la requ√™te utilisateur normalis√©e comme cl√© de cache
    if user_query:
        # Normaliser la requ√™te utilisateur (minuscules, suppression espaces multiples)
        normalized_query = ' '.join(user_query.lower().strip().split())
        cache_query = normalized_query
    else:
        # Fallback: utiliser le prompt complet si pas de user_query
        cache_query = prompt_text
    
    # V√©rifier si le cache doit √™tre invalid√© (requ√™tes similaires not√©es n√©gativement)
    if user_query and REDIS_AVAILABLE:
        try:
            from feedback_db import should_invalidate_cache
            if should_invalidate_cache(user_query):
                print("‚ö†Ô∏è Cache invalid√©: requ√™te similaire not√©e n√©gativement")
                # Invalider le cache pour cette requ√™te (Redis)
                delete_from_cache(cache_query)
        except Exception as e:
            print(f"Erreur lors de la v√©rification du cache: {e}")
    
    # Essayer de r√©cup√©rer depuis le cache Redis (partag√© entre tous les utilisateurs)
    if REDIS_AVAILABLE:
        cached_response = get_from_cache(cache_query)
        if cached_response:
            return cached_response
    
    # Fallback: utiliser le cache local (session_state) si Redis n'est pas disponible
    if not REDIS_AVAILABLE or not is_redis_available():
        local_cache = get_local_cache()
        cache_key = hashlib.sha256(cache_query.encode('utf-8')).hexdigest()
        
        if cache_key in local_cache:
            print("‚úÖ R√©ponse r√©cup√©r√©e depuis le cache local")
            return local_cache[cache_key]
    
    try:
        system_instruction = (
            "Tu es un assistant AI nomm√© Mosam con√ßu pour aider des douaniers √† troiuver les prix √† fixer sur les produits "
            "RGI 1: Les titres des sections, chapitres et sous-chapitres n'ont qu'une valeur indicative."

"RGI 2: Les marchandises incompl√®tes ou non finies sont class√©es comme compl√®tes."

"RGI 3: Le m√©lange ou l'assemblage de mati√®res ou d'articles est class√© selon la mati√®re pr√©pond√©rante."

"RGI 4: Les marchandises qui ne peuvent √™tre class√©es selon les r√®gles 1 √† 3 sont class√©es dans la position la plus analogue."

"RGI 5: Les emballages sont class√©s avec les marchandises qu'ils contiennent."

"RGI 6: Le classement des marchandises dans les sous-positions d'une m√™me position est d√©termin√© selon les termes de ces sous-positions."
            "R√©ponds uniquement aux questions pos√©es. Tu ne peux pas fournir de r√©ponse si "
            "l'information n'est pas explicitement pr√©sente dans les documents. "
            "Mosam est une √©quipe juridique, donc tu ne peux pas inventer de r√©ponse. "
            "Priorise la clart√© et la concision. "
            "Ne mentionne jamais des documents : fais comme si les infos √©taient internes. "
            "Si la question sort du p√©rim√®tre Mosam, rappelle le p√©rim√®tre. "
            "Si tu n'as pas la r√©ponse, demande des pr√©cisions. "
            "R√©ponds dans la langue du prompt suivant. "
            "Chaque r√©ponse doit, pour chaque marchandise, indiquer explicitement la position tarifaire "
            "du TEC/SH, le ou les taux d'imposition applicables (droits de douane, TVA, autres taxes "
            "si disponibles) et une justification synth√©tique (RGI, notes, crit√®res techniques). "
            "S'il y a plusieurs marchandises, structure la r√©ponse sous forme de tableau ou de liste "
            "s√©par√©e, une ligne par marchandise. "
            "Si le terme exact n'appara√Æt pas dans les documents, tente imm√©diatement plusieurs synonymes "
            "ou variantes (par exemple: \"barre m√©tallique\", \"barre en fer\", \"barre en acier\", "
            "\"produit sid√©rurgique\"). "
            "Si malgr√© ces variantes aucune mention explicite n'est trouv√©e, r√©alise une d√©duction en t'appuyant "
            "sur les RGI et sur la logique du TEC CEDEAO, en signalant clairement qu'il s'agit d'une "
            "d√©duction fond√©e sur les r√®gles."
            "Dans les documents sources, interpr√®te les abr√©viations suivantes: "
            "\"D.D.\" = droits de douane, \"R.S.\" = r√©gime statistique (taxe statistique), "
            "\"U.S.\" = unit√© de mesure et \"N.T.S.\" = num√©ro tarifaire suppl√©mentaire. "
            "Quand tu cites ces abr√©viations, ajoute toujours la d√©finition entre parenth√®ses, "
            "par exemple \"D.D. (droits de douane)\". "
            "IMPORTANT - Format de r√©ponse: " 
            "- Si la question concerne la classification d'un produit sp√©cifique, retourne exclusivement un objet JSON unique (aucun texte en dehors du JSON) respectant le sch√©ma " 
            "suivant: {\"narrative\":\"texte synth√©tique pour le douanier\",\"classifications\":[{" 
            "\"description\":\"R√©sum√© de la marchandise\",\"hs_code\":\"8517.13.00.00\"," 
            "\"section\":\"XVI\",\"chapter\":\"85\",\"dd_rate\":\"5 %\",\"rs_rate\":\"1 %\"," 
            "\"us_unit\":\"PI√àCE\",\"other_taxes\":\"TVA 18 %\"," 
            "\"justification\":\"Synth√®se RGI / crit√®res\",\"excerpt\":\"Citation exacte du document\"," 
            "\"origin\":\"USA\",\"value\":\"Non renseign√©\",\"confidence\":92}]}. " 
            "Chaque objet de \"classifications\" doit contenir au minimum ces champs; utilise " 
            "\"Non renseign√©\" si une donn√©e manque et veille √† ce que \"chapter\" soit toujours sur deux chiffres " 
            "et \"confidence\" un nombre entre 0 et 100. Le champ \"description\" doit reprendre le nom pr√©cis " 
            "du produit class√© tel qu'√©nonc√© par le douanier (ou une reformulation tr√®s courte), afin de pouvoir " 
            "l'afficher directement dans le tableau de suivi. " 
            "Si tu dois faire une d√©duction, indique-le dans \"justification\". " 
            "- Si la question est g√©n√©rale (explication, information, question sur les RGI, etc.) et ne concerne PAS la classification d'un produit sp√©cifique, " 
            "retourne un objet JSON avec uniquement {\"narrative\":\"ta r√©ponse textuelle compl√®te\",\"classifications\":[]}. " 
            "Dans ce cas, fournis une r√©ponse claire et compl√®te dans le champ \"narrative\" sans essayer de cr√©er des classifications."
        )

        client = get_openai_client()
        response = client.responses.create(
            model="gpt-5-nano",
            input=[
                {"role": "system", "content": system_instruction},
                {"role": "user", "content": prompt_text}
            ],
            store=True
        )

        response_text = response.output_text
        
        # Mettre en cache la r√©ponse (Redis partag√© ou cache local)
        if REDIS_AVAILABLE and is_redis_available():
            # Cache Redis partag√© (TTL de 7 jours par d√©faut)
            # Les r√®gles douani√®res changent rarement, donc 7 jours est appropri√©
            set_to_cache(cache_query, response_text, ttl=604800)  # 7 jours = 604800 secondes
        else:
            # Fallback: cache local (session_state)
            local_cache = get_local_cache()
            cache_key = hashlib.sha256(cache_query.encode('utf-8')).hexdigest()
            local_cache[cache_key] = response_text
            print("üíæ R√©ponse mise en cache local")
        
        return response_text

    except Exception as e:
        return f"Erreur lors de l'appel au mod√®le OpenAI : {e}"

def clear_api_cache():
    """Vide le cache des r√©ponses API (Redis et cache local)"""
    # Vider le cache Redis
    if REDIS_AVAILABLE and is_redis_available():
        deleted_count = clear_cache("cache:llm:*")
        if deleted_count > 0:
            print(f"üßπ {deleted_count} entr√©e(s) supprim√©e(s) du cache Redis")
        else:
            print("üßπ Cache Redis vid√©")
    
    # Vider le cache local (fallback)
    import streamlit as st
    if "_local_cache" in st.session_state:
        st.session_state["_local_cache"].clear()
        print("üßπ Cache local vid√©")

def get_cache_stats():
    """Retourne les statistiques du cache (Redis ou local)"""
    if REDIS_AVAILABLE and is_redis_available():
        # Statistiques Redis
        from cache_redis import get_cache_stats as get_redis_stats
        return get_redis_stats()
    else:
        # Statistiques cache local
        local_cache = get_local_cache()
        return {
            "enabled": False,
            "size": len(local_cache),
            "status": "Cache local (Redis non disponible)"
        }

def split_user_queries(raw_text):
    """D√©coupe l'entr√©e utilisateur si plusieurs articles sont fournis d'un coup."""
    if not raw_text:
        return []
    normalized = raw_text.replace("\r", "\n")
    line_parts = [
        re.sub(r"^[\-\*\d\)\.]+\s*", "", line).strip()
        for line in normalized.split("\n")
    ]
    queries = [part for part in line_parts if part]
    if len(queries) > 1:
        return queries
    if ";" in raw_text:
        semi_parts = [seg.strip() for seg in raw_text.split(";") if seg.strip()]
        if len(semi_parts) > 1:
            return semi_parts
    return [raw_text.strip()] if raw_text.strip() else []


def build_context_for_query(query, chunks, emb, index):
    """G√©n√®re un contexte documentaire pour une requ√™te pr√©cise."""
    indices, _ = search_faiss_index(query, emb, index)
    context = ""
    for idx in indices[0]:
        context += chunks[idx].page_content + "\n"
    return context


def is_general_question(user_input):
    """D√©tecte si la question est g√©n√©rale (pas de classification de produit)"""
    general_keywords = [
        "qu'est-ce que", "c'est quoi", "explique", "comment fonctionne", "quelle est la diff√©rence",
        "pourquoi", "quand", "o√π", "qui", "d√©finition", "signifie", "signification", "rgi", "r√®gles g√©n√©rales",
        "qu'est-ce qu'une", "qu'est-ce qu'un", "qu'est-ce qu'", "qu'est ce que", "qu'est ce qu'",
        "aide", "help", "assistance", "information", "informations", "explication", "expliquer"
    ]
    user_lower = user_input.lower().strip()
    
    # Si la question commence par un mot-cl√© g√©n√©ral
    for keyword in general_keywords:
        if user_lower.startswith(keyword) or f" {keyword}" in user_lower:
            return True
    
    # Si la question contient "?" et pas de nom de produit √©vident
    if "?" in user_input and len(user_input.split()) < 10:
        # V√©rifier si √ßa ressemble √† une question g√©n√©rale
        question_words = ["quoi", "comment", "pourquoi", "quand", "o√π", "qui", "quel", "quelle", "quels", "quelles"]
        first_words = user_lower.split()[:3]
        if any(word in question_words for word in first_words):
            return True
    
    return False

def process_user_input(user_input, chunks, emb, index):
    """
    Traite l'entr√©e utilisateur avec validation pr√©ventive
    """
    # D√©tecter si c'est une question g√©n√©rale
    is_general = is_general_question(user_input)
    
    if is_general:
        # Pour les questions g√©n√©rales, construire un prompt diff√©rent
        context = build_context_for_query(user_input, chunks, emb, index)
        enriched_prompt = (
            f"Question du douanier: {user_input}\n\n"
            f"Contexte documentaire disponible:\n{context}\n\n"
            "R√©ponds √† cette question de mani√®re claire et compl√®te en t'appuyant sur le contexte documentaire. "
            "Si la question ne concerne pas la classification d'un produit sp√©cifique, retourne uniquement "
            "un objet JSON avec {\"narrative\":\"ta r√©ponse textuelle compl√®te\",\"classifications\":[]}."
        )
    else:
        # Traitement normal pour les classifications de produits
        queries = split_user_queries(user_input)
        if not queries:
            return "Merci de pr√©ciser au moins une marchandise √† classifier.", None

        # VALIDATION PR√âVENTIVE: V√©rifier les feedbacks n√©gatifs similaires
        warning_message = None
        try:
            from feedback_db import check_similar_negative_feedbacks
            similar_feedbacks = check_similar_negative_feedbacks(user_input, similarity_threshold=0.6)
            
            if similar_feedbacks:
                # Construire un message d'avertissement
                feedback_count = sum(f['count'] for f in similar_feedbacks)
                warning_message = (
                    f"‚ö†Ô∏è ATTENTION: {feedback_count} requ√™te(s) similaire(s) ont re√ßu des notes n√©gatives r√©cemment. "
                    f"Veuillez v√©rifier attentivement la r√©ponse."
                )
                print(f"‚ö†Ô∏è Validation pr√©ventive: {len(similar_feedbacks)} feedback(s) n√©gatif(s) similaire(s) d√©tect√©(s)")
                
                # Ajouter un contexte d'avertissement dans le prompt
                warning_context = "\n\n‚ö†Ô∏è ATTENTION IMPORTANTE: Des requ√™tes similaires ont re√ßu des feedbacks n√©gatifs. "
                warning_context += "Sois particuli√®rement attentif √† la pr√©cision et √† la justesse de ta r√©ponse. "
                warning_context += "V√©rifie bien les codes tarifaires, les taux et les justifications.\n"
        except Exception as e:
            print(f"Erreur lors de la validation pr√©ventive: {e}")
            warning_message = None

        prompt_sections = []
        for i, query in enumerate(queries, start=1):
            context = build_context_for_query(query, chunks, emb, index)
            prompt_sections.append(
                f"[MARCHANDISE {i}]\nDescription: {query}\nContexte documentaire:\n{context}"
            )

        combined_context = "\n\n".join(prompt_sections)
        
        # Ajouter l'avertissement au prompt si n√©cessaire
        warning_prefix = warning_context if warning_message else ""
        
        enriched_prompt = (
            warning_prefix +
            "Le douanier peut avoir fourni plusieurs marchandises. "
            "Analyse chaque bloc ci-dessous et produis une r√©ponse structur√©e avec, pour chaque marchandise, "
            "la position tarifaire, le taux d'imposition et les d√©tails pertinents.\n\n"
            f"{combined_context}\n\nDemande initiale du douanier:\n{user_input}"
        )
    
    print("start the send of the question")
    # Passer user_input pour la validation du cache
    response = use_llm(enriched_prompt, user_query=user_input)
    print("finish the send of the question")
    
    # Retourner la r√©ponse avec le message d'avertissement si n√©cessaire
    # Le message d'avertissement sera g√©r√© dans app.py
    if is_general:
        return response, None  # Pas de warning pour les questions g√©n√©rales
    else:
        return response, warning_message